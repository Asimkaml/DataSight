import streamlit as st
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import io
import base64
from datetime import datetime
import re
from dotenv import load_dotenv 
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import google.generativeai as genai
import os
import openpyxl
import statsmodels 
load_dotenv()

class Backend:
    #constructor
    # function to check if gemini is available 
    def __init__(self, df):
        self.df = df
        self.df_cleaned = df.copy()
        self.numeric_columns = df.select_dtypes(include='number').columns.tolist()
        self.categorical_columns = df.select_dtypes(include='object').columns.tolist()
        self.datetime_columns = df.select_dtypes(include='datetime').columns.tolist()
        self._precompute_common_metrics()
        self.text_columns = []  # You can add text detection later
        self.data_profile = None
        self.insights = []
        self.charts = []
        self.correlation_matrix = None
         # Initialize Gemini
        # self.gemini_api_key = os.getenv("API_KEY")
        gemini_api_key = os.getenv("API_KEY")
        # gemini_api_key = st.secrets["API_KEY"]
        if gemini_api_key:
            try:
                genai.configure(api_key=gemini_api_key)
                self.gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest')
                self.gemini_available = True
            except Exception as e:
                st.warning(f"Failed to initialize Gemini: {str(e)}")
                self.gemini_available = False
        else:
            self.gemini_available = False

    # function to precompute common metrics so that they can be used later
    def _precompute_common_metrics(self):
        """Precompute metrics that are frequently needed"""
        self._numeric_stats = {
            col: {
                'mean': self.df[col].mean(),
                'median': self.df[col].median(),
                'std': self.df[col].std(),
                'min': self.df[col].min(),
                'max': self.df[col].max(),
                'skew': self.df[col].skew()
            }
            for col in self.numeric_columns
        }
        
        self._categorical_stats = {
            col: {
                'unique_count': self.df[col].nunique(),
                'top_values': self.df[col].value_counts().head(5).to_dict()
            }
            for col in self.categorical_columns
        }
        
        # Precompute correlations for numeric columns
        if len(self.numeric_columns) >= 2:
            self._correlation_matrix = self.df[self.numeric_columns].corr()

    #function to send prompt and return the response generated by gemini
    # used in generate_ai_response function: used to generate the response for the user query
    # to extract the columns from the user query
    def ask_gemini(self, prompt, max_tokens=1000):
        """Send a prompt to Gemini and get a response"""
        if not self.gemini_available:
            return "Gemini API is not available. Please provide an API key."
        
        try:
            response = self.gemini_model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"Error from Gemini: {str(e)}"

    # function to send query to gemini and get the response
    def generate_ai_response(self, user_query):
        """Generate an AI response using Gemini with improved column handling"""
        if not self.gemini_available:
            return {
                # 'text': "Gemini API is not configured. Please provide an API key in the sidebar.",
                'chart': None
            }
        
        # First try AI-assisted column extraction
        ai_columns = self._extract_columns_with_ai(user_query)
        
        # Prepare context
        context = {
            'numeric_stats': self._numeric_stats,
            'categorical_stats': self._categorical_stats,
            'sample_data': self.df.head(3).to_dict('records'),
            'suggested_columns': ai_columns if ai_columns else "No specific columns identified"
        }
        
        prompt = f"""
        Data Analysis Request: {user_query}
        
        Context:
        {str(context)}
        
        Please provide:
        1. Concise answer to the question
        2. Relevant insights
        3. Suggested visualization (if applicable) with specific column names
        
        Format visualization suggestions as:
        VISUALIZATION: <type>, <x_column>, <y_column>, <color_column>(optional)
        """
        
        try:
            response_text = self.ask_gemini(prompt)
            return self._parse_ai_response(response_text)
        except Exception as e:
            return {
                'text': f"Error processing your request: {str(e)}",
                'chart': None
            }

    def _log_query_performance(self, query_type, processing_time):
        """Track query performance for optimization"""
        if not hasattr(self, '_query_performance'):
            self._query_performance = {}
        
        if query_type not in self._query_performance:
            self._query_performance[query_type] = {
                'count': 0,
                'total_time': 0,
                'avg_time': 0
            }
        
        stats = self._query_performance[query_type]
        stats['count'] += 1
        stats['total_time'] += processing_time
        stats['avg_time'] = stats['total_time'] / stats['count']

# below function is to handle query recieved through chat
    def _handle_chat_query(self, user_query):
        """Centralized query handler with early returns"""
        query = user_query.lower()
        
        # 1. Check for correlation queries first
        if any(term in query for term in ["correlation", "relationship", "related"]):
            cols = self._extract_columns_from_query(query)
            if len(cols) >= 2:
                return self._handle_correlation_query(self.df, cols[0], cols[1])
            return {'text': "Please specify two columns for correlation analysis"}
        
        # 2. Check for top/bottom value queries
        if any(term in query for term in ["top", "highest", "max", "minimum", "lowest"]):
            col = self._extract_single_column_from_query(query)
            if col:
                return self._handle_top_values_query(self.df, col, query)
            return {'text': "Please specify a column for top/bottom analysis"}
        
        # 3. Check for time series queries
        if any(term in query for term in ["time", "trend", "over time"]):
            date_col, value_col = self._extract_time_series_columns(query)
            if date_col and value_col:
                return self._handle_time_series_query(self.df, date_col, value_col)
            return {'text': "Please specify date and value columns for time analysis"}
        
        # Fall back to Gemini if available
        if self.gemini_available:
            return self.generate_ai_response(user_query)
        
        return {
            'text': "I can help with correlations, top values, or time trends. Try asking about those.",
            'chart': None
        }
    
    #function to extract columns from the user query with the help of gemini
    def _extract_columns_with_ai(self, user_query):
        """Use AI to identify relevant columns from the user query"""
        if not self.gemini_available:
            return None
            
        prompt = f"""
        Analyze this dataset query and identify relevant columns:
        
        Dataset Columns:
        {', '.join(self.df.columns)}
        
        User Query: "{user_query}"
        
        Respond ONLY with a comma-separated list of column names from the dataset that are relevant to the query.
        If no columns are relevant, respond with 'None'.
        """
        
        try:
            response = self.ask_gemini(prompt)
            if 'none' in response.lower():
                return None
            return [col.strip() for col in response.split(',') if col.strip() in self.df.columns]
        except Exception as e:
            print(f"AI column extraction failed: {str(e)}")
            return None


    # function to parse the AI response to extract text and visualization instructions
    def _parse_ai_response(self, response_text):
        """Parse the AI response to extract text and visualization instructions"""
        chart = None
        visualization_lines = [line for line in response_text.split('\n') 
                            if line.startswith('VISUALIZATION:')]
        
        if visualization_lines:
            try:
                parts = visualization_lines[0].replace('VISUALIZATION:', '').split(',')
                parts = [p.strip() for p in parts]
                
                if len(parts) >= 2:
                    chart_type = parts[0]
                    x_col = parts[1]
                    y_col = parts[2] if len(parts) > 2 else None
                    color_col = parts[3] if len(parts) > 3 else None
                    
                    if chart_type == 'scatter' and x_col in self.df.columns and y_col in self.df.columns:
                        chart = px.scatter(
                            self.df,
                            x=x_col,
                            y=y_col,
                            color=color_col if color_col in self.df.columns else None,
                            trendline='ols' if pd.api.types.is_numeric_dtype(self.df[x_col]) and 
                                    pd.api.types.is_numeric_dtype(self.df[y_col]) else None
                        )
                    elif chart_type == 'histogram' and x_col in self.df.columns:
                        chart = px.histogram(
                            self.df,
                            x=x_col,
                            color=color_col if color_col in self.df.columns else None,
                            nbins=50
                        )
            except Exception as e:
                print(f"Failed to parse visualization suggestion: {str(e)}")
        
        return {
            'text': response_text,
            'chart': chart
        }

    # function to generate the data profile of the dataset
    def generate_data_profile(self):
        """Generate a comprehensive profile of the dataset"""
        profile = {
            'memory_usage': self.df.memory_usage(deep=True).sum() / (1024 * 1024),  # in MB
            'columns': {}
        }
        
        for col in self.df.columns:
            col_info = {
                'dtype': str(self.df[col].dtype),
                'missing_values': self.df[col].isna().sum(),
                'missing_percentage': self.df[col].isna().mean() * 100
            }
            
            if pd.api.types.is_numeric_dtype(self.df[col]):
                col_info.update({
                    'min': self.df[col].min(),
                    'max': self.df[col].max(),
                    'mean': self.df[col].mean(),
                    'median': self.df[col].median(),
                    'std': self.df[col].std(),
                    'skewness': self.df[col].skew(),
                    'kurtosis': self.df[col].kurt()
                })
            elif pd.api.types.is_categorical_dtype(self.df[col]) or pd.api.types.is_object_dtype(self.df[col]):
                col_info.update({
                    'unique_values': self.df[col].nunique(),
                    'top_values': self.df[col].value_counts().head(10).to_dict()
                })
            
            profile['columns'][col] = col_info
        
        self.data_profile = profile
        return profile
    #function to generate default visualizations for all numeric columns
    def generate_default_visualizations(self):
        """Generate basic visualizations for all numeric columns"""
        self.charts = []
        
        for col in self.numeric_columns:
            try:
                # Histogram
                fig = px.histogram(self.df, x=col, title=f"Distribution of {col}")
                self.charts.append({
                    'title': f"Histogram of {col}",
                    'description': f"Distribution of values in {col}",
                    'figure': fig
                })
                
                # Box plot
                fig = px.box(self.df, y=col, title=f"Box Plot of {col}")
                self.charts.append({
                    'title': f"Box Plot of {col}",
                    'description': f"Outlier analysis for {col}",
                    'figure': fig
                })
                
            except Exception as e:
                print(f"Could not generate visualization for {col}: {str(e)}")
        
        return self.charts

    def _handle_correlation_query(self, df, col1, col2):
        """Handle a query about correlation between two columns"""
        response = {'text': '', 'chart': None}
        
        if col1 not in df.columns or col2 not in df.columns:
            response['text'] = f"I couldn't find one of the columns in your data."
            return response
        
        # Check if both columns are numeric
        if pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):
            # Calculate correlation
            correlation = df[[col1, col2]].corr().iloc[0, 1]
            
            # Create scatter plot
            fig = px.scatter(df, x=col1, y=col2, trendline='ols' if abs(correlation) > 0.2 else None)
            
            fig.update_layout(
                title=f'Relationship between {col1} and {col2}',
                xaxis_title=col1,
                yaxis_title=col2
            )
            
            # Describe the correlation
            if abs(correlation) < 0.3:
                strength = "weak or no"
            elif abs(correlation) < 0.7:
                strength = "moderate"
            else:
                strength = "strong"
            
            direction = "positive" if correlation >= 0 else "negative"
            
            response['text'] = f"There is a {strength} {direction} correlation ({correlation:.2f}) between {col1} and {col2}. "
            if correlation > 0.4:
                response['text'] += f"This means that as {col1} increases, {col2} tends to increase as well."
            elif correlation < 0:
                response['text'] += f"This means that as {col1} increases, {col2} tends to decrease."
            
            response['chart'] = fig
        
        else:
            response['text'] = f"I can only calculate correlation between numeric columns. "
            if not pd.api.types.is_numeric_dtype(df[col1]):
                response['text'] += f"'{col1}' is not numeric. "
            if not pd.api.types.is_numeric_dtype(df[col2]):
                response['text'] += f"'{col2}' is not numeric. "
        
        return response

    def _handle_top_values_query(self, df, column, query):
        """Handle a query about top or bottom values in a column"""
        response = {'text': '', 'chart': None}
        
        if column not in df.columns:
            response['text'] = f"I couldn't find a column named {column} in your data."
            return response
        
        # Determine if we need top or bottom values
        ascending = 'lowest' in query.lower() or 'minimum' in query.lower()
        n_values = 10  # Default number of values to show
        
        # Try to extract a number from the query (e.g., "top 5 values")
        match = re.search(r'top\s+(\d+)', query.lower())
        if match:
            n_values = int(match.group(1))
        
        if pd.api.types.is_numeric_dtype(df[column]):
            # For numeric columns, sort and display top/bottom values
            sorted_df = df.sort_values(by=column, ascending=ascending).head(n_values)
            
            # Create a bar chart
            fig = go.Figure(go.Bar(
                x=sorted_df.index,
                y=sorted_df[column],
                text=sorted_df[column],
                textposition='auto'
            ))
            
            direction = "Lowest" if ascending else "Highest"
            fig.update_layout(
                title=f'{direction} {n_values} values of {column}',
                xaxis_title="Index",
                yaxis_title=column
            )
            
            response['text'] = f"Here are the {direction.lower()} {n_values} values for {column}:"
            for idx, val in enumerate(sorted_df[column]):
                response['text'] += f"\n{idx+1}. {val:.2f}"
            
            response['chart'] = fig
        
        else:
            # For categorical columns, show frequency counts
            value_counts = df[column].value_counts(ascending=ascending).head(n_values)
            
            fig = go.Figure(go.Bar(
                x=value_counts.index,
                y=value_counts.values,
                text=value_counts.values,
                textposition='auto'
            ))
            
            direction = "Least" if ascending else "Most"
            fig.update_layout(
                title=f'{direction} common values in {column}',
                xaxis_title=column,
                yaxis_title="Count"
            )
            
            response['text'] = f"Here are the {direction.lower()} common values for {column}:"
            for idx, (val, count) in enumerate(value_counts.items()):
                response['text'] += f"\n{idx+1}. '{val}' ({count} occurrences)"
            
            response['chart'] = fig
        
        return response

    def _handle_time_series_query(self, df, date_col, value_col):
        """Handle a query about changes over time"""
        response = {'text': '', 'chart': None}
        
        if date_col not in df.columns or value_col not in df.columns:
            response['text'] = f"I couldn't find one of the columns in your data."
            return response
        
        # Ensure date column is in datetime format
        if not pd.api.types.is_datetime64_any_dtype(df[date_col]):
            try:
                df[date_col] = pd.to_datetime(df[date_col])
            except:
                response['text'] = f"I couldn't convert '{date_col}' to a date format."
                return response
        
        # Only keep rows with valid dates and values
        temp_df = df[[date_col, value_col]].dropna().copy()
        
        # Sort by date
        temp_df = temp_df.sort_values(by=date_col)
        
        # Check if we have enough data
        if len(temp_df) < 2:
            response['text'] = f"There isn't enough data to analyze changes over time."
            return response
        
        # Determine appropriate time grouping
        date_range = (temp_df[date_col].max() - temp_df[date_col].min()).days
        
        if date_range <= 30:
            # Daily data
            temp_df['period'] = temp_df[date_col].dt.date
            period_name = "day"
        elif date_range <= 365:
            # Monthly data
            temp_df['period'] = temp_df[date_col].dt.strftime('%Y-%m')
            period_name = "month"
        else:
            # Yearly data
            temp_df['period'] = temp_df[date_col].dt.year
            period_name = "year"
        
        # Aggregate by period
        grouped = temp_df.groupby('period')[value_col].agg(['mean', 'min', 'max', 'count'])
        
        # Create time series figure
        fig = go.Figure()
        
        # Add mean line
        fig.add_trace(go.Scatter(
            x=grouped.index,
            y=grouped['mean'],
            name=f'Average {value_col}',
            line=dict(color='blue', width=2)
        ))
        
        # Add min-max range
        fig.add_trace(go.Scatter(
            x=grouped.index.tolist() + grouped.index.tolist()[::-1],
            y=grouped['max'].tolist() + grouped['min'].tolist()[::-1],
            fill='toself',
            fillcolor='rgba(0, 176, 246, 0.2)',
            line=dict(color='rgba(255, 255, 255, 0)'),
            name='Min-Max Range'
        ))
        
        fig.update_layout(
            title=f'Changes in {value_col} over time',
            xaxis_title='Time Period',
            yaxis_title=value_col,
            legend_title='Metrics'
        )
        
        # Analyze trends
        first_mean = grouped['mean'].iloc[0]
        last_mean = grouped['mean'].iloc[-1]
        percent_change = ((last_mean - first_mean) / first_mean * 100) if first_mean != 0 else 0
        
        # Detect if trend is increasing, decreasing, or flat
        if abs(percent_change) < 5:
            trend = "remained relatively stable"
        elif percent_change > 0:
            trend = f"increased by approximately {percent_change:.1f}%"
        else:
            trend = f"decreased by approximately {abs(percent_change):.1f}%"
        
        response['text'] = f"Analysis of {value_col} over time:\n\n"
        response['text'] += f"The average {value_col} {trend} from {grouped.index[0]} to {grouped.index[-1]}. "
        
        # Add count information
        total_count = grouped['count'].sum()
        response['text'] += f"\n\nThis analysis includes {total_count} data points across {len(grouped)} time periods. "
        
        # Check for seasonality or cyclical patterns
        if len(grouped) >= 12 and period_name == "month":
            # Simple seasonality check by extracting month and averaging by month
            try:
                temp_df['month'] = pd.to_datetime(temp_df['period']).dt.month
                monthly_avg = temp_df.groupby('month')[value_col].mean()
                
                max_month = monthly_avg.idxmax()
                min_month = monthly_avg.idxmin()
                
                month_names = {
                    1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',
                    7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'
                }
                
                response['text'] += f"\n\nThere appears to be some seasonality, with highest values typically in {month_names[max_month]} and lowest in {month_names[min_month]}."
            except:
                # Skip seasonality check if it fails
                pass
        
        response['chart'] = fig
        
        return response

    def generate_report(self, title="Data Analysis Report", include_sections=None):
        """Generate a comprehensive report of the data analysis"""
        if include_sections is None:
            include_sections = {
                'overview': True,
                'profile': True,
                'insights': True,
                'visualizations': True
            }
        
        report = {
            'title': title,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'sections': []
        }
        
        # Dataset Overview Section
        if include_sections.get('overview', True):
            overview_section = {
                'title': 'Dataset Overview',
                'content': f"The dataset contains {self.df.shape[0]} rows and {self.df.shape[1]} columns.",
                'items': [
                    {
                        'type': 'text',
                        'content': f"- Number of rows: {self.df.shape[0]}"
                    },
                    {
                        'type': 'text',
                        'content': f"- Number of columns: {self.df.shape[1]}"
                    },
                    {
                        'type': 'text',
                        'content': f"- Memory usage: {self.data_profile.get('memory_usage', 0):.2f} MB"
                    },
                    {
                        'type': 'text',
                        'content': f"- Missing values: {self.df.isna().sum().sum()} ({self.df.isna().sum().sum() / (self.df.shape[0] * self.df.shape[1]) * 100:.2f}%)"
                    },
                    {
                        'type': 'table',
                        'caption': 'Data Sample',
                        'data': self.df.head(5).to_html(index=False)
                    }
                ]
            }
            report['sections'].append(overview_section)
        
        # Data Profile Section
        if include_sections.get('profile', True):
            profile_section = {
                'title': 'Data Profile',
                'content': "Detailed information about each column in the dataset.",
                'items': []
            }
            
            # Column types summary
            column_types = {
                'Numeric': len(self.numeric_columns),
                'Categorical': len(self.categorical_columns),
                'Datetime': len(self.datetime_columns),
                'Text': len(self.text_columns)
            }
            
            profile_section['items'].append({
                'type': 'text',
                'content': "Column Types Summary:"
            })
            
            for col_type, count in column_types.items():
                if count > 0:
                    profile_section['items'].append({
                        'type': 'text',
                        'content': f"- {col_type}: {count} columns"
                    })
            
            # Add column details
            if "columns" in self.data_profile:
                for col_name, col_info in self.data_profile["columns"].items():
                    col_content = f"### {col_name}\n"
                    col_content += f"- Type: {col_info.get('dtype', 'unknown')}\n"
                    col_content += f"- Missing Values: {col_info.get('missing_values', 0)} ({col_info.get('missing_percentage', 0)}%)\n"
                    
                    if "min" in col_info:  # Numeric column
                        col_content += f"- Min: {col_info.get('min', 0)}\n"
                        col_content += f"- Max: {col_info.get('max', 0)}\n"
                        col_content += f"- Mean: {col_info.get('mean', 0)}\n"
                        col_content += f"- Median: {col_info.get('median', 0)}\n"
                        col_content += f"- Standard Deviation: {col_info.get('std', 0)}\n"
                    
                    if "unique_values" in col_info:  # Categorical column
                        col_content += f"- Unique Values: {col_info.get('unique_values', 0)}\n"
                        if "top_values" in col_info:
                            col_content += "- Top Values:\n"
                            for val, count in list(col_info["top_values"].items())[:5]:  # Show top 5
                                col_content += f"  - {val}: {count}\n"
                    
                    profile_section['items'].append({
                        'type': 'text',
                        'content': col_content
                    })
                    
                    # Try to add a mini visualization based on column type
                    if col_name in self.numeric_columns:
                        try:
                            # Create histogram
                            plt.figure(figsize=(4, 3))
                            sns.histplot(self.df[col_name].dropna(), kde=True)
                            plt.title(f"Distribution of {col_name}")
                            
                            # Convert plot to base64 image
                            buffer = BytesIO()
                            plt.savefig(buffer, format='png')
                            buffer.seek(0)
                            image_png = buffer.getvalue()
                            buffer.close()
                            plt.close()
                            
                            image_b64 = base64.b64encode(image_png).decode('utf-8')
                            
                            profile_section['items'].append({
                                'type': 'image',
                                'caption': f"Distribution of {col_name}",
                                'data': f"data:image/png;base64,{image_b64}"
                            })
                        except:
                            # Skip if visualization fails
                            pass
            
            report['sections'].append(profile_section)
        
        # Insights Section
        if include_sections.get('insights', True) and self.insights:
            insights_section = {
                'title': 'Key Insights',
                'content': "Automatically generated insights from the data.",
                'items': []
            }
            
            for insight in self.insights:
                insights_section['items'].append({
                    'type': 'insight',
                    'title': insight.get('title', 'Insight'),
                    'content': insight.get('description', ''),
                    'importance': insight.get('importance', 'medium')
                })
            
            report['sections'].append(insights_section)
        
        # Visualizations Section
        if include_sections.get('visualizations', True) and self.charts:
            viz_section = {
                'title': 'Data Visualizations',
                'content': "Automatically generated visualizations to explore the data.",
                'items': []
            }
            
            for i, chart in enumerate(self.charts):
                try:
                    # Convert Plotly figure to image
                    fig = chart.get('figure')
                    if fig:
                        img_bytes = fig.to_image(format="png")
                        img_b64 = base64.b64encode(img_bytes).decode('utf-8')
                        
                        viz_section['items'].append({
                            'type': 'image',
                            'caption': chart.get('title', f'Chart {i+1}'),
                            'description': chart.get('description', ''),
                            'data': f"data:image/png;base64,{img_b64}"
                        })
                except:
                    # Skip if conversion fails
                    continue
            
            report['sections'].append(viz_section)
        
        # Recommendations Section
        recommendations_section = {
            'title': 'Recommendations',
            'content': "Based on the analysis, here are some recommended next steps.",
            'items': []
        }
        
        # Generate simple recommendations based on insights and profile
        recommendations = []
        
        # Check for missing values
        missing_cols = []
        for col, info in self.data_profile.get('columns', {}).items():
            if info.get('missing_percentage', 0) > 5:
                missing_cols.append((col, info.get('missing_percentage', 0)))
        
        if missing_cols:
            missing_cols.sort(key=lambda x: x[1], reverse=True)
            col, pct = missing_cols[0]
            recommendations.append(f"Address missing values in column '{col}' ({pct:.1f}% missing) by investigating the data collection process or using advanced imputation techniques.")
        
        # Check for outliers
        outlier_insights = [insight for insight in self.insights if 'outlier' in insight.get('title', '').lower()]
        if outlier_insights:
            recommendations.append("Investigate outliers identified in the analysis, as they may distort statistical measures or indicate data quality issues.")
        
        # Check for correlations
        corr_insights = [insight for insight in self.insights if 'correlation' in insight.get('title', '').lower()]
        if corr_insights:
            recommendations.append("Explore the strong correlations identified to better understand relationships in your data and potential causal factors.")
        
        # Time series recommendations
        if self.datetime_columns:
            recommendations.append("Consider time series forecasting methods to predict future trends based on the temporal patterns identified in the data.")
        
        # Add recommendations to the report
        for rec in recommendations:
            recommendations_section['items'].append({
                'type': 'text',
                'content': f"- {rec}"
            })
        
        report['sections'].append(recommendations_section)
        
        return report

    def export_report_html(self, report):
        """Export the report as HTML"""
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>{report['title']}</title>
            <style>
                body {{
                    font-family: Arial, sans-serif;
                    line-height: 1.6;
                    color: #333;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                }}
                h1 {{
                    color: #1E88E5;
                    border-bottom: 2px solid #1E88E5;
                    padding-bottom: 10px;
                }}
                h2 {{
                    color: #424242;
                    margin-top: 30px;
                    padding-bottom: 5px;
                    border-bottom: 1px solid #ddd;
                }}
                .timestamp {{
                    color: #757575;
                    font-style: italic;
                    margin-bottom: 30px;
                }}
                .section {{
                    margin-bottom: 40px;
                }}
                .insight-card {{
                    background-color: #f0f8ff;
                    padding: 15px;
                    border-radius: 5px;
                    border-left: 5px solid #1E88E5;
                    margin-bottom: 15px;
                }}
                .high {{ border-left-color: #ff6b6b; }}
                .medium {{ border-left-color: #f9a825; }}
                .low {{ border-left-color: #66bb6a; }}
                table {{
                    border-collapse: collapse;
                    width: 100%;
                    margin: 15px 0;
                }}
                th, td {{
                    text-align: left;
                    padding: 8px;
                    border: 1px solid #ddd;
                }}
                th {{
                    background-color: #f2f2f2;
                }}
                img {{
                    max-width: 100%;
                    height: auto;
                    display: block;
                    margin: 20px 0;
                    border: 1px solid #ddd;
                }}
                .image-caption {{
                    text-align: center;
                    margin-top: 5px;
                    font-style: italic;
                    color: #666;
                }}
            </style>
        </head>
        <body>
            <h1>{report['title']}</h1>
            <div class="timestamp">Generated on {report['timestamp']}</div>
        """
        
        # Add each section
        for section in report['sections']:
            html += f"""
            <div class="section">
                <h2>{section['title']}</h2>
                <p>{section['content']}</p>
            """
            
            # Add section items
            for item in section['items']:
                item_type = item.get('type', 'text')
                
                if item_type == 'text':
                    html += f"<p>{item['content']}</p>"
                    
                elif item_type == 'table':
                    html += f"""
                    <div>
                        <p>{item.get('caption', '')}</p>
                        {item['data']}
                    </div>
                    """
                    
                elif item_type == 'image':
                    html += f"""
                    <div>
                        <img src="{item['data']}" alt="{item.get('caption', 'Chart')}">
                        <div class="image-caption">{item.get('caption', '')}</div>
                        <p>{item.get('description', '')}</p>
                    </div>
                    """
                    
                elif item_type == 'insight':
                    html += f"""
                    <div class="insight-card {item.get('importance', 'medium')}">
                        <h3>{item['title']}</h3>
                        <p>{item['content']}</p>
                    </div>
                    """
            
            html += "</div>"
        
        html += """
        </body>
        </html>
        """
        
        return html

    def gaussian_kde(self, x):
        """Simple Gaussian KDE implementation as a fallback"""
        # This is a simplified version in case scipy is not available
        from numpy import exp, subtract, square, sqrt, pi, mean, linspace
        
        bandwidth = 1.06 * x.std() * len(x)**(-0.2)  # Silverman's rule of thumb
        
        def kde_func(eval_points):
            result = []
            for point in eval_points:
                kernels = exp(-0.5 * square(subtract(x, point)) / square(bandwidth)) / (bandwidth * sqrt(2 * pi))
                result.append(mean(kernels))
            return result
        
        return kde_func
    def generate_insights(self):
        """Generate automatic insights about the data"""
        self.insights = []  # Clear previous insights
    
    # Generate insights based on data profile
        if self.data_profile is None:
          self.generate_data_profile()
    
    # Example insight - missing values
        for col, info in self.data_profile['columns'].items():
          if info['missing_percentage'] > 20:
            self.insights.append({
                'title': f"High missing values in {col}",
                'description': f"Column '{col}' has {info['missing_percentage']:.1f}% missing values which may affect analysis.",
                'importance': 'high'
            })
    
    # Example insight - skewed distributions
        for col in self.numeric_columns:
         skew = self.df[col].skew()
         if abs(skew) > 1:
            self.insights.append({
                'title': f"Skewed distribution in {col}",
                'description': f"Column '{col}' is highly skewed ({skew:.2f}). Consider transformations for better analysis.",
                'importance': 'medium'
            })
    
    # Example insight - potential outliers
        for col in self.numeric_columns:
         q1 = self.df[col].quantile(0.25)
         q3 = self.df[col].quantile(0.75)
         iqr = q3 - q1
         outlier_count = ((self.df[col] < (q1 - 1.5 * iqr)) | (self.df[col] > (q3 + 1.5 * iqr))).sum()
         if outlier_count > 0:
            self.insights.append({
                'title': f"Potential outliers in {col}",
                'description': f"Column '{col}' has {outlier_count} potential outliers using the IQR method.",
                'importance': 'medium'
            })
    
        return self.insights
